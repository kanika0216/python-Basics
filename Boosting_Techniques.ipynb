{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMT3msYXyaX1Bqe/Wh18D8d",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kanika0216/python-Basics/blob/main/Boosting_Techniques.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Theoretical**\n",
        "\n",
        "Que 1: What is Boosting in Machine Learning\n",
        "\n",
        "Ans: Boosting is a machine learning ensemble technique that combines multiple weak learners sequentially to form a strong learner by focusing on the errors of previous models.\n",
        "\n",
        "Que 2: How does Boosting differ from Bagging\n",
        "\n",
        "Ans: Boosting trains models sequentially with a focus on correcting errors made by previous models, while Bagging trains models in parallel and combines their outputs to reduce variance.\n",
        "\n",
        "Que 3: What is the key idea behind AdaBoost\n",
        "\n",
        "Ans: The key idea of AdaBoost is to combine several weak classifiers into one strong classifier by adjusting weights on training data based on previous errors.\n",
        "\n",
        "Que 4: Explain the working of AdaBoost with an example\n",
        "\n",
        "Ans: AdaBoost starts by assigning equal weights to all data points. After each model, weights increase for misclassified points and decrease for correctly classified ones. For example, if a sample is misclassified, it gets more focus in the next iteration to improve accuracy.\n",
        "\n",
        "Que 5: What is Gradient Boosting, and how is it different from AdaBoost\n",
        "\n",
        "Ans: Gradient Boosting builds models sequentially to minimize a loss function using gradient descent, whereas AdaBoost focuses on adjusting weights based on classification errors.\n",
        "\n",
        "Que 6: What is the loss function in Gradient Boosting\n",
        "\n",
        "Ans: The loss function in Gradient Boosting is a differentiable function that measures the difference between actual and predicted values, commonly mean squared error or log loss.\n",
        "\n",
        "Que 7: How does XGBoost improve over traditional Gradient Boosting\n",
        "\n",
        "Ans: XGBoost improves by using regularization, parallel processing, tree pruning, and optimized handling of missing values, making it faster and more accurate.\n",
        "\n",
        "Que 8: What is the difference between XGBoost and CatBoost\n",
        "\n",
        "Ans: XGBoost is efficient but needs preprocessing for categorical data, while CatBoost handles categorical features natively and reduces overfitting through ordered boosting.\n",
        "\n",
        "Que 9: What are some real-world applications of Boosting techniques\n",
        "\n",
        "Ans: Boosting is used in fraud detection, spam filtering, customer churn prediction, medical diagnosis, and ranking algorithms in search engines.\n",
        "\n",
        "Que 10: How does regularization help in XGBoost\n",
        "\n",
        "Ans: Regularization in XGBoost controls model complexity and prevents overfitting by penalizing large coefficients and complex trees.\n",
        "\n",
        "Que 11: What are some hyperparameters to tune in Gradient Boosting models\n",
        "\n",
        "Ans: Key hyperparameters include learning rate, number of estimators, max depth, subsample, and loss function.\n",
        "\n",
        "Que 12: What is the concept of Feature Importance in Boosting\n",
        "\n",
        "Ans: Feature Importance indicates how much a feature contributes to the predictive power of the model, helping in feature selection and interpretation.\n",
        "\n",
        "Que 13: Why is CatBoost efficient for categorical data?\n",
        "\n",
        "Ans: CatBoost efficiently handles categorical variables using ordered boosting and permutation-driven techniques, avoiding the need for one-hot encoding."
      ],
      "metadata": {
        "id": "7AsBMbMMCDLs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Practical**"
      ],
      "metadata": {
        "id": "YQUzAjYyCQXK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Que 14: Train an AdaBoost Classifier on a sample dataset and print model accuracy"
      ],
      "metadata": {
        "id": "2Txh6kilCU1G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model = AdaBoostClassifier(n_estimators=50)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "cddjU8l3CQtb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Que 15: Train an AdaBoost Regressor and evaluate performance using Mean Absolute Error (MAE)"
      ],
      "metadata": {
        "id": "saMhSiB6CQ0n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import AdaBoostRegressor\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "X, y = make_regression(n_samples=1000, n_features=20, noise=0.1, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model = AdaBoostRegressor(n_estimators=50)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "print(\"MAE:\", mean_absolute_error(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "n7RWhWswCQ73"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Que 16: Train a Gradient Boosting Classifier on the Breast Cancer dataset and print feature importance"
      ],
      "metadata": {
        "id": "247L8BJRCRB3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.2, random_state=42)\n",
        "\n",
        "model = GradientBoostingClassifier(n_estimators=100)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "importances = model.feature_importances_\n",
        "plt.barh(data.feature_names, importances)\n",
        "plt.xlabel(\"Feature Importance\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Rz6feRSMCRI7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Que 17: Train a Gradient Boosting Regressor and evaluate using R-Squared Score"
      ],
      "metadata": {
        "id": "JvMsISDkCRPL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "X, y = make_regression(n_samples=1000, n_features=20, noise=0.1)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "model = GradientBoostingRegressor()\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "print(\"R2 Score:\", r2_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "8gchzQCFCRUL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Que 18: Train an XGBoost Classifier on a dataset and compare accuracy with Gradient Boosting"
      ],
      "metadata": {
        "id": "rhF0H_LQCRX6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X, y = make_classification(n_samples=1000, n_features=20)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
        "xgb.fit(X_train, y_train)\n",
        "xgb_pred = xgb.predict(X_test)\n",
        "\n",
        "gb = GradientBoostingClassifier()\n",
        "gb.fit(X_train, y_train)\n",
        "gb_pred = gb.predict(X_test)\n",
        "\n",
        "print(\"XGBoost Accuracy:\", accuracy_score(y_test, xgb_pred))\n",
        "print(\"Gradient Boosting Accuracy:\", accuracy_score(y_test, gb_pred))\n"
      ],
      "metadata": {
        "id": "gsX7efNvCRcN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Que 19: Train a CatBoost Classifier and evaluate using F1-Score"
      ],
      "metadata": {
        "id": "H7k92A2jCRf6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from catboost import CatBoostClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "X, y = make_classification(n_samples=1000, n_features=20)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "model = CatBoostClassifier(verbose=0)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "print(\"F1 Score:\", f1_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "nD6GAHqICRlq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Que 20: Train an XGBoost Regressor and evaluate using Mean Squared Error (MSE)"
      ],
      "metadata": {
        "id": "JHKhQNgBCRp6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBRegressor\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "X, y = make_regression(n_samples=1000, n_features=20, noise=0.1)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "model = XGBRegressor()\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "print(\"MSE:\", mean_squared_error(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "FCz6aZQbCRu9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Que 21: Train an AdaBoost Classifier and visualize feature importance"
      ],
      "metadata": {
        "id": "tf9-yMthCRya"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "X, y = make_classification(n_samples=1000, n_features=10)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "model = AdaBoostClassifier(n_estimators=100)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "plt.bar(range(len(model.feature_importances_)), model.feature_importances_)\n",
        "plt.xlabel(\"Feature Index\")\n",
        "plt.ylabel(\"Importance\")\n",
        "plt.title(\"AdaBoost Feature Importance\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "XamGzDJVCR3g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Que 22: Train a Gradient Boosting Regressor and plot learning curves"
      ],
      "metadata": {
        "id": "GVRjrBoGCR7x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.model_selection import learning_curve\n",
        "from sklearn.datasets import make_regression\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "X, y = make_regression(n_samples=1000, n_features=20, noise=0.1)\n",
        "model = GradientBoostingRegressor()\n",
        "\n",
        "train_sizes, train_scores, test_scores = learning_curve(model, X, y, cv=5)\n",
        "\n",
        "train_scores_mean = np.mean(train_scores, axis=1)\n",
        "test_scores_mean = np.mean(test_scores, axis=1)\n",
        "\n",
        "plt.plot(train_sizes, train_scores_mean, label=\"Training score\")\n",
        "plt.plot(train_sizes, test_scores_mean, label=\"Cross-validation score\")\n",
        "plt.xlabel(\"Training Size\")\n",
        "plt.ylabel(\"Score\")\n",
        "plt.title(\"Learning Curve\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "XQ2KlnlMCR_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Que 23: Train an XGBoost Classifier and visualize feature importance"
      ],
      "metadata": {
        "id": "Vn1_ig-6CSFI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBClassifier, plot_importance\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "X, y = make_classification(n_samples=1000, n_features=20)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "model = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "plot_importance(model)\n",
        "plt.title(\"XGBoost Feature Importance\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "tcOqdufYCSJm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Que 24: Train a CatBoost Classifier and plot the confusion matrix"
      ],
      "metadata": {
        "id": "eOkXhVFgCSNn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from catboost import CatBoostClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "X, y = make_classification(n_samples=1000, n_features=20)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "model = CatBoostClassifier(verbose=0)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "disp.plot()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "PzgDJDx8CSSa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Que 25: Train an AdaBoost Classifier with different numbers of estimators and compare accuracy"
      ],
      "metadata": {
        "id": "j2sXAn6wCSWG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X, y = make_classification(n_samples=1000, n_features=20)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "for n in [10, 50, 100]:\n",
        "    model = AdaBoostClassifier(n_estimators=n)\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    print(f\"Accuracy with {n} estimators:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "nNNLHsBtCSam"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Que 26: Train a Gradient Boosting Classifier and visualize the ROC curve"
      ],
      "metadata": {
        "id": "RDtRbLSWCSeX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "X, y = make_classification(n_samples=1000, n_features=20)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "model = GradientBoostingClassifier()\n",
        "model.fit(X_train, y_train)\n",
        "y_score = model.predict_proba(X_test)[:,1]\n",
        "\n",
        "fpr, tpr, _ = roc_curve(y_test, y_score)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "plt.plot(fpr, tpr, label='ROC Curve (AUC = %0.2f)' % roc_auc)\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "eZ6uO6OLCSjG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Que 27: Train an XGBoost Regressor and tune the learning rate using GridSearchCV"
      ],
      "metadata": {
        "id": "stWgRScRCSnM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBRegressor\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "\n",
        "X, y = make_regression(n_samples=1000, n_features=20, noise=0.1)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "param_grid = {'learning_rate': [0.01, 0.05, 0.1, 0.2]}\n",
        "model = XGBRegressor()\n",
        "grid = GridSearchCV(model, param_grid, cv=3)\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best learning rate:\", grid.best_params_['learning_rate'])\n"
      ],
      "metadata": {
        "id": "9a8vootACSsL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Que 28: Train a CatBoost Classifier on an imbalanced dataset and compare performance with class weighting"
      ],
      "metadata": {
        "id": "CMCZwZWnCSwa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from catboost import CatBoostClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "X, y = make_classification(n_samples=1000, weights=[0.9, 0.1], n_classes=2)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "# Without class weights\n",
        "model1 = CatBoostClassifier(verbose=0)\n",
        "model1.fit(X_train, y_train)\n",
        "print(\"Without class weights:\")\n",
        "print(classification_report(y_test, model1.predict(X_test)))\n",
        "\n",
        "# With class weights\n",
        "model2 = CatBoostClassifier(class_weights=[1, 10], verbose=0)\n",
        "model2.fit(X_train, y_train)\n",
        "print(\"With class weights:\")\n",
        "print(classification_report(y_test, model2.predict(X_test)))\n"
      ],
      "metadata": {
        "id": "6EbA13qZCS09"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Que 29: Train an AdaBoost Classifier and analyze the effect of different learning rates"
      ],
      "metadata": {
        "id": "1J5Lm07NCS46"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X, y = make_classification(n_samples=1000, n_features=20)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "for lr in [0.01, 0.1, 0.5, 1]:\n",
        "    model = AdaBoostClassifier(learning_rate=lr)\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    print(f\"Accuracy with learning rate {lr}:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "P0HRoc0gCS9L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Que 30: Train an XGBoost Classifier for multi-class classification and evaluate using log-loss"
      ],
      "metadata": {
        "id": "g7ZkitlDCTBM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_classes=3, n_informative=10)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "model = XGBClassifier(objective='multi:softprob', num_class=3, eval_metric='mlogloss')\n",
        "model.fit(X_train, y_train)\n",
        "y_pred_proba = model.predict_proba(X_test)\n",
        "\n",
        "print(\"Log Loss:\", log_loss(y_test, y_pred_proba))\n"
      ],
      "metadata": {
        "id": "8gx9rE5iCTFd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}