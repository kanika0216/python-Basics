{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPFrnD5LiGhVmoOu3bInOdb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kanika0216/python-Basics/blob/main/SVM_%26_Naive_bayes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Theoretical**"
      ],
      "metadata": {
        "id": "SI74ekju6Vo1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.  What is a Support Vector Machine (SVM)?\n",
        "\n",
        "Support Vector Machine (SVM) is a supervised learning algorithm used for classification and regression tasks. It works by finding the optimal hyperplane that best separates different classes in a dataset, maximizing the margin between them.\n",
        "\n",
        "\n",
        "\n",
        "2. What is the difference between Hard Margin and Soft Margin SVM?\n",
        "\n",
        "Hard Margin SVM: Only works when data is linearly separable. It strictly enforces that no points from different classes overlap.  \n",
        "Soft Margin SVM: Allows some misclassification by introducing a penalty for misclassified points, making it more robust for real-world data that may have noise.\n",
        "\n",
        "\n",
        "3. What is the mathematical intuition behind SVM?\n",
        "\n",
        "SVM aims to maximize the **margin** (distance between the closest points of different classes, called support vectors) while minimizing misclassification. The optimization problem involves minimizing \\( ||w||^2 \\) subject to correct classification constraints. It is solved using **Lagrange multipliers** and **quadratic programming**.\n",
        "\n",
        "\n",
        "4. What is the role of Lagrange Multipliers in SVM?\n",
        "\n",
        "Lagrange multipliers help in transforming the constrained optimization problem of SVM into an unconstrained problem. This allows the model to find the optimal decision boundary while considering only **support vectors** rather than the entire dataset.\n",
        "\n",
        "\n",
        "\n",
        "5. What are Support Vectors in SVM?\n",
        "\n",
        "Support vectors are the data points closest to the decision boundary (hyperplane). They are the most critical points in defining the classifier, as they determine the margin width.\n",
        "\n",
        "\n",
        "6. What is a Support Vector Classifier (SVC)?\n",
        "\n",
        "A Support Vector Classifier (SVC) is an extension of SVM that applies to datasets where classes are not completely separable. It introduces a **soft margin** to allow misclassification and improve generalization.\n",
        "\n",
        "\n",
        "\n",
        "7. What is a Support Vector Regressor (SVR)?\n",
        "\n",
        "Support Vector Regressor (SVR) applies SVM principles to regression problems. Instead of maximizing the margin between classes, it tries to fit a function within a specified error margin (\\(\\epsilon\\)).\n",
        "\n",
        "\n",
        "\n",
        "8. What is the Kernel Trick in SVM?\n",
        "\n",
        "The **Kernel Trick** allows SVM to operate in a **higher-dimensional space** without explicitly transforming the data. It uses kernel functions to compute the dot product in this higher space, enabling SVM to classify data that is not linearly separable.\n",
        "\n",
        "\n",
        "\n",
        "9. Compare Linear Kernel, Polynomial Kernel, and RBF Kernel\n",
        "\n",
        "- **Linear Kernel**: Best for linearly separable data. Simple and efficient.  \n",
        "- **Polynomial Kernel**: Captures more complex relationships, but can be computationally expensive.  \n",
        "- **RBF (Radial Basis Function) Kernel**: Maps data into infinite-dimensional space, making it highly flexible for non-linear data.\n",
        "\n",
        "\n",
        "\n",
        "10. HB: What is the effect of the C parameter in SVM?\n",
        "\n",
        "The **C parameter** controls the trade-off between margin size and misclassification.  \n",
        "- **High C** → Smaller margin, fewer misclassifications (risk of overfitting).  \n",
        "- **Low C** → Larger margin, more misclassification allowed (better generalization).  \n",
        "\n",
        "\n",
        "\n",
        "11. What is the role of the Gamma parameter in RBF Kernel SVM?\n",
        "\n",
        "Gamma (\\(\\gamma\\)) defines how far the influence of a single training example reaches.  \n",
        "- **High Gamma** → Each training point has a close influence (may overfit).  \n",
        "- **Low Gamma** → More general decision boundary (better generalization).  \n",
        "\n",
        "\n",
        "\n",
        "12. What is the Naïve Bayes classifier, and why is it called \"Naïve\"?\n",
        "\n",
        "Naïve Bayes is a probabilistic classifier based on **Bayes' Theorem**, assuming that features are **independent** given the class label. It is called \"naïve\" because this independence assumption is often unrealistic in real-world data.\n",
        "\n",
        "\n",
        "\n",
        "13.  What is Bayes’ Theorem?\n",
        "\n",
        "Bayes' Theorem describes the probability of an event based on prior knowledge of related conditions:  \n",
        "\\[\n",
        "P(A|B) = \\frac{P(B|A) P(A)}{P(B)}\n",
        "\\]  \n",
        "Where:  \n",
        "- \\( P(A|B) \\) = Posterior probability of A given B  \n",
        "- \\( P(B|A) \\) = Likelihood  \n",
        "- \\( P(A) \\) = Prior probability  \n",
        "- \\( P(B) \\) = Evidence  \n",
        "\n",
        "\n",
        "\n",
        "14. Explain the differences between Gaussian Naïve Bayes, Multinomial Naïve Bayes, and Bernoulli Naïve Bayes\n",
        "\n",
        "- **Gaussian Naïve Bayes**: Used for **continuous** data (assumes a normal distribution).  \n",
        "- **Multinomial Naïve Bayes**: Used for **text classification** (word counts).  \n",
        "- **Bernoulli Naïve Bayes**: Used for **binary** features (e.g., presence/absence of words in text data).  \n",
        "\n",
        "\n",
        "\n",
        "15.  When should you use Gaussian Naïve Bayes over other variants?\n",
        "\n",
        "Gaussian Naïve Bayes is ideal when working with **continuous numerical features** that follow a normal (Gaussian) distribution, such as **height, weight, or temperature**.\n",
        "\n",
        "\n",
        "\n",
        "16. What are the key assumptions made by Naïve Bayes?\n",
        "\n",
        " **Feature Independence**: All features are conditionally independent given the class.  \n",
        " **Equal Contribution**: Every feature contributes equally to the final classification.  \n",
        " **Class Conditional Independence**: The effect of a feature is independent of other features.  \n",
        "\n",
        "\n",
        "\n",
        "17.  What are the advantages and disadvantages of Naïve Bayes?\n",
        "\n",
        "**Advantages:**  \n",
        "- Fast and efficient, even with large datasets.  \n",
        "- Works well for text classification.  \n",
        "- Requires small training data to estimate parameters.  \n",
        "\n",
        "**Disadvantages:**  \n",
        "- Assumes **feature independence**, which is often unrealistic.  \n",
        "- Struggles with features that are highly correlated.  \n",
        "\n",
        "\n",
        "\n",
        "18. Why is Naïve Bayes a good choice for text classification?\n",
        "\n",
        "Naïve Bayes performs well for text classification because:  \n",
        "- It handles **high-dimensional** data efficiently.  \n",
        "- The **independence assumption** is reasonable for words in a document.  \n",
        "- It requires **low computational power**.  \n",
        "\n",
        "\n",
        "\n",
        "19.  Compare SVM and Naïve Bayes for classification tasks\n",
        "\n",
        "| Feature | SVM | Naïve Bayes |  \n",
        "|---------|-----|-------------|  \n",
        "| Works well with | Large feature space | High-dimensional text data |  \n",
        "| Assumptions | No independence assumption | Assumes feature independence |  \n",
        "| Speed | Slower for large datasets | Very fast and efficient |  \n",
        "| Robustness | More complex, better accuracy | Works well with small data |  \n",
        "| Overfitting | Tuned using C & gamma | Less prone to overfitting |  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "20. How does Laplace Smoothing help in Naïve Bayes?\n",
        "\n",
        "Laplace Smoothing prevents **zero probability issues** when a word is absent in training data but appears in testing. It modifies probability estimation by adding a smoothing parameter (\\(\\alpha\\)):\n",
        "\n",
        "\n",
        "\\[P(word | class) = \\frac{\\text{word count} + \\alpha}{\\text{total words} + \\alpha \\times \\text{vocabulary size}}\\]  \n",
        "\n",
        "\n",
        "Where **\\(\\alpha = 1\\)** is commonly used.  \n",
        "\n"
      ],
      "metadata": {
        "id": "Ufu5o4M26Z2F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Practical**"
      ],
      "metadata": {
        "id": "i2TfftSJ8mZi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Write a Python program to train an SVM Classifier on the Iris dataset and evaluate accuracy"
      ],
      "metadata": {
        "id": "9G-kmXCB8pVR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = datasets.load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "svm_model = SVC(kernel='linear')\n",
        "svm_model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = svm_model.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"SVM Classifier Accuracy on Iris Dataset: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nlc-9wyI9vVP",
        "outputId": "76aff7dc-b787-478f-fce1-bd6034e3502c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM Classifier Accuracy on Iris Dataset: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Write a Python program to train two SVM classifiers with Linear and RBF kernels on the Wine dataset, then\n",
        "compare their accuracies"
      ],
      "metadata": {
        "id": "xznnSxNI9xlt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "wine = datasets.load_wine()\n",
        "X, y = wine.data, wine.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "linear_svm = SVC(kernel='linear')\n",
        "rbf_svm = SVC(kernel='rbf')\n",
        "\n",
        "linear_svm.fit(X_train, y_train)\n",
        "rbf_svm.fit(X_train, y_train)\n",
        "\n",
        "linear_acc = accuracy_score(y_test, linear_svm.predict(X_test))\n",
        "rbf_acc = accuracy_score\n"
      ],
      "metadata": {
        "id": "Fvo50l9P-DXa"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Write a Python program to train an SVM Regressor (SVR) on a housing dataset and evaluate it using Mean\n",
        "Squared Error (MSE)"
      ],
      "metadata": {
        "id": "UFaTDWOx-Eua"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.datasets import fetch_openml\n",
        "\n",
        "housing = fetch_openml(name='house_prices', version=1)\n",
        "X, y = housing.data, housing.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "svr = SVR()\n",
        "svr.fit(X_train, y_train)\n",
        "\n",
        "y_pred = svr.predict(X_test)\n",
        "\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"SVR Mean Squared Error: {mse:.2f}\")\n"
      ],
      "metadata": {
        "id": "Lq8ZRVst-IfS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Write a Python program to train an SVM Classifier with a Polynomial Kernel and visualize the decision\n",
        "boundary"
      ],
      "metadata": {
        "id": "J02qEL56-K9u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "iris = load_iris()\n",
        "X, y = iris.data[:, :2], iris.target  # Using only two features for visualization\n",
        "\n",
        "model = SVC(kernel='poly', degree=3)\n",
        "model.fit(X, y)\n",
        "\n",
        "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100))\n",
        "Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "Z = Z.reshape(xx.shape)\n",
        "\n",
        "plt.contourf(xx, yy, Z, alpha=0.3)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "N76TnWRH-N9A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Write a Python program to train a Gaussian Naïve Bayes classifier on the Breast Cancer dataset and\n",
        "evaluate accuracy"
      ],
      "metadata": {
        "id": "ChH5V86U-U8U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "cancer = load_breast_cancer()\n",
        "X, y = cancer.data, cancer.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "nb_model = GaussianNB()\n",
        "nb_model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = nb_model.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Gaussian Naïve Bayes Accuracy on Breast Cancer Dataset: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "id": "-bXjTvLz-XwR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Write a Python program to train a Multinomial Naïve Bayes classifier for text classification using the 20\n",
        "Newsgroups dataset."
      ],
      "metadata": {
        "id": "JjufH-KH-a3Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "newsgroups = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\n",
        "X_train, X_test, y_train, y_test = train_test_split(newsgroups.data, newsgroups.target, test_size=0.2, random_state=42)\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "nb_model = MultinomialNB()\n",
        "nb_model.fit(X_train_tfidf, y_train)\n",
        "\n",
        "y_pred = nb_model.predict(X_test_tfidf)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Multinomial Naïve Bayes Accuracy on 20 Newsgroups Dataset: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "id": "C08eC0Nw-e0B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Write a Python program to train an SVM Classifier with different C values and compare the decision\n",
        "boundaries visually"
      ],
      "metadata": {
        "id": "rFOtA7sO-hPe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "iris = load_iris()\n",
        "X, y = iris.data[:, :2], iris.target\n",
        "\n",
        "C_values = [0.1, 1, 10]\n",
        "for C in C_values:\n",
        "    model = SVC(kernel='linear', C=C)\n",
        "    model.fit(X, y)\n",
        "\n",
        "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "    y_min, y_max = X[:, 1\n"
      ],
      "metadata": {
        "id": "ZcMrgQs0-kmq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Write a Python program to train a Bernoulli Naïve Bayes classifier for binary classification on a dataset with\n",
        "binary features."
      ],
      "metadata": {
        "id": "yCzWoPV3-0rt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=0, n_clusters_per_class=1, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "bnb_model = BernoulliNB()\n",
        "bnb_model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = bnb_model.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Bernoulli Naïve Bayes Accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "id": "Dwv1nFS5-4Te"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Write a Python program to apply feature scaling before training an SVM model and compare results with\n",
        "unscaled data"
      ],
      "metadata": {
        "id": "Uw1vckh2-6LL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "svm_model = SVC(kernel='linear')\n",
        "svm_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "y_pred_scaled = svm_model.predict(X_test_scaled)\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "svm_model.fit(X_train, y_train)\n",
        "y_pred_unscaled = svm_model.predict(X_test)\n",
        "accuracy_unscaled = accuracy_score(y_test, y_pred_unscaled)\n",
        "\n",
        "print(f\"SVM Accuracy with Scaled Data: {accuracy_scaled:.2f}\")\n",
        "print(f\"SVM Accuracy with Unscaled Data: {accuracy_unscaled:.2f}\")\n"
      ],
      "metadata": {
        "id": "S1NhEt-x-8mK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Write a Python program to train a Gaussian Naïve Bayes model and compare the predictions before and\n",
        "after Laplace Smoothing"
      ],
      "metadata": {
        "id": "_2cll5za--Ra"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "cancer = load_breast_cancer()\n",
        "X, y = cancer.data, cancer.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "nb_model_no_smoothing = GaussianNB(var_smoothing=0)\n",
        "nb_model_no_smoothing.fit(X_train, y_train)\n",
        "\n",
        "y_pred_no_smoothing = nb_model_no_smoothing.predict(X_test)\n",
        "accuracy_no_smoothing = accuracy_score(y_test, y_pred_no_smoothing)\n",
        "\n",
        "nb_model_with_smoothing = GaussianNB()\n",
        "nb_model_with_smoothing.fit(X_train, y_train)\n",
        "\n",
        "y_pred_with_smoothing = nb_model_with_smoothing.predict(X_test)\n",
        "accuracy_with_smoothing = accuracy_score(y_test, y_pred_with_smoothing)\n",
        "\n",
        "print(f\"Accuracy without Laplace Smoothing: {accuracy_no_smoothing:.2f}\")\n",
        "print(f\"Accuracy with Laplace Smoothing: {accuracy_with_smoothing:.2f}\")\n"
      ],
      "metadata": {
        "id": "XsVICHUD_BXP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. Write a Python program to train an SVM Classifier and use GridSearchCV to tune the hyperparameters (C,\n",
        "gamma, kernel)"
      ],
      "metadata": {
        "id": "udggIMF8_DwM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "param_grid = {'C': [0.1, 1, 10], 'gamma': ['scale', 'auto'], 'kernel': ['linear', 'rbf']}\n",
        "grid_search = GridSearchCV(SVC(), param_grid, cv=5)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "print(f\"Best Parameters: {grid_search.best_params_}\")\n"
      ],
      "metadata": {
        "id": "xT6gVnOF_Gm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. Write a Python program to train an SVM Classifier on an imbalanced dataset and apply class weighting and\n",
        "check it improve accuracy"
      ],
      "metadata": {
        "id": "L7q49Mfa_IjP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=0, weights=[0.9, 0.1], random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "svm_model = SVC(kernel='linear', class_weight='balanced')\n",
        "svm_model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = svm_model.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"SVM Classifier with Class Weighting Accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "id": "0wVdSc1Z_L6x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. Write a Python program to implement a Naïve Bayes classifier for spam detection using email data"
      ],
      "metadata": {
        "id": "GPflnhna_NsQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Example email data (for demonstration purposes, replace with actual email dataset)\n",
        "emails = ['Free money now!', 'Meeting at 3pm', 'Limited time offer', 'Let\\'s catch up tomorrow']\n",
        "labels = [1, 0, 1, 0]  # 1: Spam, 0: Not Spam\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(emails)\n",
        "y = labels\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "nb_model = MultinomialNB()\n",
        "nb_model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = nb_model.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Naïve Bayes Spam Detection Accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "id": "X2ZQIq0X_RVt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. Write a Python program to train an SVM Classifier and a Naïve Bayes Classifier on the same dataset and\n",
        "compare their accuracy"
      ],
      "metadata": {
        "id": "0QEn1pBm_TNc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "svm_model = SVC(kernel='linear')\n",
        "svm_model.fit(X_train, y_train)\n",
        "svm_acc = accuracy_score(y_test, svm_model.predict(X_test))\n",
        "\n",
        "nb_model = GaussianNB()\n",
        "nb_model.fit(X_train, y_train)\n",
        "nb_acc = accuracy_score(y_test, nb_model.predict(X_test))\n",
        "\n",
        "print(f\"SVM Classifier Accuracy: {svm_acc:.2f}\")\n",
        "print(f\"Naïve Bayes Classifier Accuracy: {nb_acc:.2f}\")\n"
      ],
      "metadata": {
        "id": "pOhMRVHZ_Vpq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. Write a Python program to perform feature selection before training a Naïve Bayes classifier and compare\n",
        "results"
      ],
      "metadata": {
        "id": "zEiiX-CZ_X0-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "selector = SelectKBest(f_classif, k=2)\n",
        "X_train_selected = selector.fit_transform(X_train, y_train)\n",
        "X_test_selected = selector.transform(X_test)\n",
        "\n",
        "nb_model = GaussianNB()\n",
        "nb_model.fit(X_train_selected, y_train)\n",
        "\n",
        "y_pred_selected = nb_model.predict(X_test_selected)\n",
        "accuracy_selected = accuracy_score(y_test, y_pred_selected)\n",
        "\n",
        "nb_model.fit(X_train, y_train)\n",
        "y_pred_full = nb_model.predict(X_test)\n",
        "accuracy_full = accuracy_score(y_test, y_pred_full)\n",
        "\n",
        "print(f\"Naïve Bayes Accuracy with Feature Selection: {accuracy_selected:.2f}\")\n",
        "print(f\"Naïve Bayes Accuracy without Feature Selection: {accuracy_full:.2f}\")\n"
      ],
      "metadata": {
        "id": "CsKVqNjL_bFL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. Write a Python program to train an SVM Classifier using One-vs-Rest (OvR) and One-vs-One (OvO)\n",
        "strategies on the Wine dataset and compare their accuracy"
      ],
      "metadata": {
        "id": "fvQQGQVq_dOk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "wine = load_wine()\n",
        "X, y = wine.data, wine.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "ovr_model = SVC(kernel='linear', decision_function_shape='ovr')\n",
        "ovo_model = SVC(kernel='linear', decision_function_shape='ovo')\n",
        "\n",
        "ovr_model.fit(X_train, y_train)\n",
        "ovo_model.fit(X_train, y_train)\n",
        "\n",
        "ovr_acc = accuracy_score(y_test, ovr_model.predict(X_test))\n",
        "ovo_acc = accuracy_score(y_test, ovo_model.predict(X_test))\n",
        "\n",
        "print(f\"One-vs-Rest Accuracy: {ovr_acc:.2f}\")\n",
        "print(f\"One-vs-One Accuracy: {ovo_acc:.2f}\")\n"
      ],
      "metadata": {
        "id": "bWE-fX8e_fyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. Write a Python program to train an SVM Classifier using Linear, Polynomial, and RBF kernels on the Breast\n",
        "Cancer dataset and compare their accuracy"
      ],
      "metadata": {
        "id": "W5vaI8iE_h5l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "cancer = load_breast_cancer()\n",
        "X, y = cancer.data, cancer.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "kernels = ['linear', 'poly', 'rbf']\n",
        "for kernel in kernels:\n",
        "    model = SVC(kernel=kernel)\n",
        "    model.fit(X_train, y_train)\n",
        "    accuracy = accuracy_score(y_test, model.predict(X_test))\n",
        "    print(f\"Accuracy with {kernel} kernel: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "id": "hFYHkZ3i_lGs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. Write a Python program to train an SVM Classifier using Stratified K-Fold Cross-Validation and compute the\n",
        "average accuracy"
      ],
      "metadata": {
        "id": "t5rBDOZX_ogA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "svm_model = SVC(kernel='linear')\n",
        "cv_scores = cross_val_score(svm_model, X, y, cv=5)\n",
        "\n",
        "print(f\"Average Accuracy with Stratified K-Fold Cross-Validation: {cv_scores.mean():.2f}\")\n"
      ],
      "metadata": {
        "id": "jHWwIkX3_vF-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. Write a Python program to train a Naïve Bayes classifier using different prior probabilities and compare\n",
        "performance"
      ],
      "metadata": {
        "id": "3SQXiD6r_xJX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "prior_probabilities = [np.array([0.3, 0.4, 0.3]), np.array([0.2, 0.5, 0.3]), np.array([0.4, 0.4, 0.2])]\n",
        "for priors in prior_probabilities:\n",
        "    nb_model = GaussianNB(priors=priors)\n",
        "    nb_model.fit(X_train, y_train)\n",
        "    y_pred = nb_model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\"Naïve Bayes Accuracy with priors {priors}: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "id": "36IAukA2_zqC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. Write a Python program to perform Recursive Feature Elimination (RFE) before training an SVM Classifier and\n",
        "compare accuracy"
      ],
      "metadata": {
        "id": "iGPyvB_F_1hS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "svm_model = SVC(kernel='linear')\n",
        "rfe = RFE(svm_model, n_features_to_select=2)\n",
        "X_train_rfe = rfe.fit_transform(X_train, y_train)\n",
        "X_test_rfe = rfe.transform(X_test)\n",
        "\n",
        "svm_model.fit(X_train_rfe, y_train)\n",
        "y_pred_rfe = svm_model.predict(X_test_rfe)\n",
        "accuracy_rfe = accuracy_score(y_test, y_pred_rfe)\n",
        "\n",
        "svm_model.fit(X_train, y_train)\n",
        "y_pred_full = svm_model.predict(X_test)\n",
        "accuracy_full = accuracy_score(y_test, y_pred_full)\n",
        "\n",
        "print(f\"SVM Accuracy with RFE: {accuracy_rfe:.2f}\")\n",
        "print(f\"SVM Accuracy without RFE: {accuracy_full:.2f}\")\n"
      ],
      "metadata": {
        "id": "LJ-jE9lC_39T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "21. Write a Python program to train an SVM Classifier and evaluate its performance using Precision, Recall, and\n",
        "F1-Score instead of accuracy"
      ],
      "metadata": {
        "id": "NesM-B7h_5ei"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "cancer = load_breast_cancer()\n",
        "X, y = cancer.data, cancer.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "svm_model = SVC(kernel='linear')\n",
        "svm_model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = svm_model.predict(X_test)\n",
        "\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "print(f\"F1-Score: {f1:.2f}\")\n"
      ],
      "metadata": {
        "id": "UF5U_wji_73K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "22. Write a Python program to train a Naïve Bayes Classifier and evaluate its performance using Log Loss\n",
        "(Cross-Entropy Loss)."
      ],
      "metadata": {
        "id": "YPkxjvLn_9s7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "cancer = load_breast_cancer()\n",
        "X, y = cancer.data, cancer.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "nb_model = GaussianNB()\n",
        "nb_model.fit(X_train, y_train)\n",
        "\n",
        "y_pred_prob = nb_model.predict_proba(X_test)\n",
        "loss = log_loss(y_test, y_pred_prob)\n",
        "\n",
        "print(f\"Log Loss (Cross-Entropy Loss): {loss:.2f}\")\n"
      ],
      "metadata": {
        "id": "oJNFQq1xAAbN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "23. Write a Python program to train an SVM Classifier and visualize the Confusion Matrix using seaborn"
      ],
      "metadata": {
        "id": "dWRHIEw6ADOA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "cancer = load_breast_cancer()\n",
        "X, y = cancer.data, cancer.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "svm_model = SVC(kernel='linear')\n",
        "svm_model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = svm_model.predict(X_test)\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=cancer.target_names, yticklabels=cancer.target_names)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "I-wYdmGQAGYX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "24. Write a Python program to train an SVM Regressor (SVR) and evaluate its performance using Mean Absolute\n",
        "Error (MAE) instead of MSE"
      ],
      "metadata": {
        "id": "B0siE9WrAH_g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVR\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "housing = fetch_openml(name='house_prices', version=1)\n",
        "X, y = housing.data, housing.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "svr = SVR()\n",
        "svr.fit(X_train, y_train)\n",
        "\n",
        "y_pred = svr.predict(X_test)\n",
        "\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "print(f\"SVR Mean Absolute Error: {mae:.2f}\")\n"
      ],
      "metadata": {
        "id": "kAFbj8-AAKeC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "25. Write a Python program to train a Naïve Bayes classifier and evaluate its performance using the ROC-AUC\n",
        "score"
      ],
      "metadata": {
        "id": "r3Vbtpi9ANeX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "cancer = load_breast_cancer()\n",
        "X, y = cancer.data, cancer.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "nb_model = GaussianNB()\n",
        "nb_model.fit(X_train, y_train)\n",
        "\n",
        "y_pred_prob = nb_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "roc_auc = roc_auc_score(y_test, y_pred_prob)\n",
        "print(f\"ROC-AUC Score: {roc_auc:.2f}\")\n"
      ],
      "metadata": {
        "id": "t-C9ZQw8AQc-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "26. Write a Python program to train an SVM Classifier and visualize the Precision-Recall Curve."
      ],
      "metadata": {
        "id": "FqO_vohbASFR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "cancer = load_breast_cancer()\n",
        "X, y = cancer.data, cancer.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "svm_model = SVC(kernel='linear', probability=True)\n",
        "svm_model.fit(X_train, y_train)\n",
        "\n",
        "y_pred_prob = svm_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "precision, recall, _ = precision_recall_curve(y_test, y_pred_prob)\n",
        "\n",
        "plt.plot(recall, precision, color='blue')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "5ftfbjaCAU1s"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}