{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOhWiQTm/SWzRBsHZ/q8NTd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kanika0216/python-Basics/blob/main/Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. **What is Simple Linear Regression?**  \n",
        "   It is a statistical method that models the relationship between a dependent variable (Y) and an independent variable (X) using the equation \\( Y = mX + c \\).  \n",
        "\n",
        "2. **Key Assumptions of Simple Linear Regression?**  \n",
        "   - Linearity between X and Y  \n",
        "   - Independence of observations  \n",
        "   - Homoscedasticity (constant variance of residuals)  \n",
        "   - Normality of residuals  \n",
        "   - No or minimal multicollinearity  \n",
        "\n",
        "3. **What does the coefficient m represent in \\( Y = mX + c \\)?**  \n",
        "   The slope \\( m \\) represents the change in Y for a one-unit increase in X.  \n",
        "\n",
        "4. **What does the intercept c represent in \\( Y = mX + c \\)?**  \n",
        "   The intercept \\( c \\) is the predicted value of Y when X = 0.  \n",
        "\n",
        "5. **How do we calculate the slope \\( m \\) in Simple Linear Regression?**  \n",
        "   \\( m = \\frac{\\sum (X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum (X_i - \\bar{X})^2} \\).  \n",
        "\n",
        "6. **What is the purpose of the least squares method?**  \n",
        "   It minimizes the sum of squared differences between observed and predicted values to find the best-fitting line.  \n",
        "\n",
        "7. **How is the coefficient of determination (R²) interpreted?**  \n",
        "   R² measures the proportion of variance in Y explained by X, ranging from 0 to 1.  \n",
        "\n",
        "8. **What is Multiple Linear Regression?**  \n",
        "   It extends Simple Linear Regression to multiple independent variables:  \n",
        "   \\( Y = b_0 + b_1X_1 + b_2X_2 + ... + b_nX_n \\).  \n",
        "\n",
        "9. **Main difference between Simple and Multiple Linear Regression?**  \n",
        "   Simple has one independent variable, while Multiple has two or more.  \n",
        "\n",
        "10. **Key assumptions of Multiple Linear Regression?**  \n",
        "   - Linearity  \n",
        "   - Independence of errors  \n",
        "   - Homoscedasticity  \n",
        "   - Normality of residuals  \n",
        "   - No multicollinearity  \n",
        "\n",
        "11. **What is heteroscedasticity, and how does it affect results?**  \n",
        "   It refers to non-constant variance of residuals, which can distort statistical significance.  \n",
        "\n",
        "12. **How to improve a model with high multicollinearity?**  \n",
        "   - Remove highly correlated variables  \n",
        "   - Use Principal Component Analysis (PCA)  \n",
        "   - Apply Ridge or Lasso Regression  \n",
        "\n",
        "13. **Common techniques for transforming categorical variables?**  \n",
        "   - One-hot encoding  \n",
        "   - Label encoding  \n",
        "   - Dummy variables  \n",
        "\n",
        "14. **Role of interaction terms in Multiple Linear Regression?**  \n",
        "   They capture the combined effect of two independent variables on Y.  \n",
        "\n",
        "15. **How can the interpretation of intercept differ between Simple and Multiple Linear Regression?**  \n",
        "   In Simple, it represents Y when X = 0, but in Multiple, it represents Y when all Xs = 0.  \n",
        "\n",
        "16. **Significance of the slope and its effect on predictions?**  \n",
        "   A significant slope means X has a strong effect on Y, impacting predictions accordingly.  \n",
        "\n",
        "17. **How does the intercept provide context for the relationship?**  \n",
        "   It gives a baseline value of Y when all Xs are zero.  \n",
        "\n",
        "18. **Limitations of using R² as a sole measure of model performance?**  \n",
        "   R² doesn’t indicate causation or model overfitting; adjusted R² is more reliable.  \n",
        "\n",
        "19. **Interpretation of a large standard error for a coefficient?**  \n",
        "   It suggests high variability, meaning the coefficient estimate is less reliable.  \n",
        "\n",
        "20. **How to identify heteroscedasticity in residual plots and why address it?**  \n",
        "   Unequal spread of residuals in a scatter plot signals heteroscedasticity, which can bias results.  \n",
        "\n",
        "21. **Meaning of high R² but low adjusted R²?**  \n",
        "   The model has too many predictors, some of which may not contribute to explaining Y.  \n",
        "\n",
        "22. **Why is scaling important in Multiple Linear Regression?**  \n",
        "   Scaling ensures fair comparisons between variables and improves convergence in optimization algorithms.  \n",
        "\n",
        "23. **What is polynomial regression?**  \n",
        "   A regression technique where the relationship is modeled as a polynomial equation, e.g., \\( Y = a + bX + cX^2 \\).  \n",
        "\n",
        "24. **How does polynomial regression differ from linear regression?**  \n",
        "   It models nonlinear relationships by adding polynomial terms.  \n",
        "\n",
        "25. **When is polynomial regression used?**  \n",
        "   When the data shows a curved rather than a straight-line trend.  \n",
        "\n",
        "26. **General equation for polynomial regression?**  \n",
        "   \\( Y = b_0 + b_1X + b_2X^2 + ... + b_nX^n \\).  \n",
        "\n",
        "27. **Can polynomial regression be applied to multiple variables?**  \n",
        "   Yes, it can include polynomial terms for multiple independent variables.  \n",
        "\n",
        "28. **Limitations of polynomial regression?**  \n",
        "   Prone to overfitting and sensitive to outliers.  \n",
        "\n",
        "29. **Methods to evaluate model fit when selecting polynomial degree?**  \n",
        "   - Cross-validation  \n",
        "   - Adjusted R²  \n",
        "   - AIC/BIC scores  \n",
        "\n",
        "30. **Why is visualization important in polynomial regression?**  \n",
        "   It helps in identifying the right polynomial degree and avoiding overfitting.  \n",
        "\n",
        "31. **How is polynomial regression implemented in Python?**  \n",
        "   Using `PolynomialFeatures` from `sklearn.preprocessing` and `LinearRegression`.  \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QA6Ks7kyv2Kg"
      }
    }
  ]
}