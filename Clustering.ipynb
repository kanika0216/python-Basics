{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP3BC1oWuxxirVc/ThhH9jE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kanika0216/python-Basics/blob/main/Clustering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Theoretical Questions**"
      ],
      "metadata": {
        "id": "DgVndWDgJBfG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques 1: What is unsupervised learning in the context of machine learning\n",
        "\n",
        "Ans\n",
        "Unsupervised learning is a type of machine learning where the algorithm learns patterns and structure from data without any labeled responses. It is mainly used for clustering and dimensionality reduction.\n",
        "\n",
        "Ques 2: How does K-Means clustering algorithm work\n",
        "\n",
        "Ans\n",
        "K-Means clustering partitions data into K clusters by randomly initializing centroids, assigning data points to the nearest centroid, recalculating centroids based on current clusters, and repeating until centroids stabilize or a maximum number of iterations is reached.\n",
        "\n",
        "Ques 3: Explain the concept of a dendrogram in hierarchical clustering\n",
        "\n",
        "Ans\n",
        "A dendrogram is a tree-like diagram that shows how individual data points are merged into clusters step by step in hierarchical clustering. Each merge is represented by a horizontal line connecting the merged clusters. The height of the line indicates the distance or dissimilarity between the merged clusters. By cutting the dendrogram at a certain height, we can decide the number of clusters.\n",
        "\n",
        "Ques 4: What is the main difference between K-Means and Hierarchical Clustering\n",
        "\n",
        "Ans\n",
        "The main difference is that K-Means requires the number of clusters to be defined in advance and partitions the data directly, whereas Hierarchical Clustering builds a hierarchy of clusters without needing the number of clusters beforehand, using a tree-like structure.\n",
        "\n",
        "Ques 5: What are the advantages of DBSCAN over K-Means\n",
        "\n",
        "Ans\n",
        "DBSCAN can find clusters of arbitrary shapes and is more robust to noise and outliers, unlike K-Means which assumes spherical clusters and is sensitive to outliers.\n",
        "\n",
        "Ques 6: When would you use Silhouette Score in clustering\n",
        "\n",
        "Ans\n",
        "Silhouette Score is used to measure the quality of clustering. It helps determine how similar an object is to its own cluster compared to other clusters and is commonly used to select the optimal number of clusters.\n",
        "\n",
        "Ques 7: What are the limitations of Hierarchical Clustering\n",
        "\n",
        "Ans\n",
        "Hierarchical Clustering has high computational complexity, does not perform well with large datasets, and once a merge or split is done, it cannot be undone, making it inflexible.\n",
        "\n",
        "Ques 8: Why is feature scaling important in clustering algorithms like K-Means\n",
        "\n",
        "Ans\n",
        "Feature scaling ensures that all features contribute equally to distance calculations. Without it, variables with larger ranges dominate the clustering process, leading to biased results.\n",
        "\n",
        "Ques 9: How does DBSCAN identify noise points\n",
        "\n",
        "Ans\n",
        "DBSCAN labels points as noise if they do not have enough neighboring points within a specified radius (epsilon) and are not reachable from any dense region.\n",
        "\n",
        "Ques 10: Define inertia in the context of K-Means\n",
        "\n",
        "Ans\n",
        "Inertia is the sum of squared distances between each data point and its assigned cluster centroid. Lower inertia indicates tighter clusters.\n",
        "\n",
        "Ques 11: What is the elbow method in K-Means clustering\n",
        "\n",
        "Ans\n",
        "The elbow method involves plotting the inertia against a range of K values and selecting the point where the decrease in inertia slows down significantly, resembling an \"elbow\" shape.\n",
        "\n",
        "Ques 12: Describe the concept of \"density\" in DBSCAN\n",
        "\n",
        "Ans\n",
        "Density in DBSCAN refers to the number of data points within a given radius. A point is considered part of a cluster if there are enough points (defined by min_samples) within its epsilon neighborhood.\n",
        "\n",
        "Ques 13: Can hierarchical clustering be used on categorical data\n",
        "\n",
        "Ans\n",
        "Yes, hierarchical clustering can be applied to categorical data using appropriate distance measures like Hamming distance or by converting categories into binary indicators.\n",
        "\n",
        "Ques 14: What does a negative Silhouette Score indicate\n",
        "\n",
        "Ans\n",
        "A negative Silhouette Score indicates that a sample is likely placed in the wrong cluster, as its average distance to points in its own cluster is higher than to points in a neighboring cluster.\n",
        "\n",
        "Ques 15: Explain the term \"linkage criteria\" in hierarchical clustering\n",
        "\n",
        "Ans\n",
        "Linkage criteria determine how distances between clusters are calculated when merging. Common types include single linkage (minimum distance), complete linkage (maximum distance), and average linkage (average distance between points in the clusters).\n",
        "\n",
        "Ques 16: Why might K-Means clustering perform poorly on data with varying cluster sizes or densities\n",
        "\n",
        "Ans\n",
        "K-Means assumes equal-sized, spherical clusters. It performs poorly on data with clusters of varying sizes or densities because it uses Euclidean distance, which does not handle such variations well.\n",
        "\n",
        "Ques 17: What are the core parameters in DBSCAN, and how do they influence clustering\n",
        "\n",
        "Ans\n",
        "The core parameters are eps (radius of neighborhood) and min_samples (minimum number of points in a neighborhood to form a core point). These parameters influence how dense regions are defined and how many clusters or noise points are identified.\n",
        "\n",
        "Ques 18: How does K-Means++ improve upon standard K-Means initialization\n",
        "\n",
        "Ans\n",
        "K-Means++ improves initialization by choosing initial centroids that are far apart, which leads to better and more consistent clustering results and reduces the chances of poor local minima.\n",
        "\n",
        "Ques 19: What is agglomerative clustering\n",
        "\n",
        "Ans\n",
        "Agglomerative clustering is a bottom-up approach in hierarchical clustering where each data point starts as its own cluster, and pairs of clusters are merged based on similarity until one cluster remains or a stopping criterion is met.\n",
        "\n",
        "Ques 20: What makes Silhouette Score a better metric than just inertia for model evaluation\n",
        "\n",
        "Ans\n",
        "Silhouette Score considers both intra-cluster cohesion and inter-cluster separation, providing a more comprehensive measure of cluster quality compared to inertia, which only evaluates compactness."
      ],
      "metadata": {
        "id": "h2FTx0AiJEQL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Practical Questions:**"
      ],
      "metadata": {
        "id": "XSxOW5kIKKGh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques 21: Generate synthetic blobs with 5 centers and apply KMeans. Then use silhouette_score to evaluate the clustering\n",
        "\n",
        "Ans\n",
        "Use make_blobs(n_samples=500, centers=5) to create synthetic data. Apply KMeans(n_clusters=5) and fit it to the data. Then calculate silhouette_score(X, kmeans.labels_) to evaluate clustering quality. A higher silhouette score indicates better-defined clusters.\n",
        "\n",
        "Ques 22: Load the Breast Cancer dataset, reduce dimensionality using PCA, and apply Agglomerative Clustering. Visualize in 2D\n",
        "\n",
        "Ans\n",
        "Load the Breast Cancer dataset, standardize it, and reduce dimensions to 2 using PCA. Apply AgglomerativeClustering(n_clusters=2) and visualize the clusters in a 2D scatter plot using the two principal components.\n",
        "\n",
        "Ques 23: Generate noisy circular data using make_circles and visualize clustering results from KMeans and DBSCAN side-by-side\n",
        "\n",
        "Ans\n",
        "Generate circular data using make_circles(noise=0.05). Fit both KMeans(n_clusters=2) and DBSCAN(eps=0.2, min_samples=5) to the data. Use subplots to show both clustering outputs side-by-side. DBSCAN handles the circular shape better.\n",
        "\n",
        "Ques 24: Load the Iris dataset and plot the Silhouette Coefficient for each sample after KMeans clustering\n",
        "\n",
        "Ans\n",
        "Load and standardize the Iris dataset. Apply KMeans(n_clusters=3). Use silhouette_samples() to get the silhouette score for each sample and plot them using a bar graph to assess individual sample clustering quality.\n",
        "\n",
        "Ques 25: Generate synthetic data using make_blobs and apply Agglomerative Clustering with 'average' linkage. Visualize clusters\n",
        "\n",
        "Ans\n",
        "Create data using make_blobs. Apply AgglomerativeClustering(n_clusters=3, linkage='average'). Visualize the clusters using a scatter plot, coloring by predicted labels to see how average linkage merges clusters based on average distances.\n",
        "\n",
        "Ques 26: Load the Wine dataset, apply KMeans, and visualize the cluster assignments in a seaborn pairplot (first 4 features)\n",
        "\n",
        "Ans\n",
        "Load and scale the Wine dataset. Apply KMeans(n_clusters=3) and add the labels to the dataframe. Use sns.pairplot() on the first 4 features with the cluster labels as hue to visualize how the model grouped the samples.\n",
        "\n",
        "Ques 27: Generate noisy blobs using make_blobs and use DBSCAN to identify both clusters and noise points. Print the count\n",
        "\n",
        "Ans\n",
        "Use make_blobs and optionally add noise manually. Fit DBSCAN with suitable eps and min_samples. Count the number of clusters by checking np.unique(labels) and count noise points as those labeled -1.\n",
        "\n",
        "Ques 28: Load the Digits dataset, reduce dimensions using t-SNE, then apply Agglomerative Clustering and plot the clusters\n",
        "\n",
        "Ans\n",
        "Load the Digits dataset and reduce dimensions to 2 using TSNE(n_components=2). Fit AgglomerativeClustering(n_clusters=10) and plot the results with each digit cluster shown in a different color.\n",
        "\n",
        "Ques 29: Generate synthetic data with 4 centers using make_blobs and apply K-Means clustering. Visualize using a scatter plot\n",
        "\n",
        "Ans\n",
        "Generate data with make_blobs(n_samples=500, centers=4). Apply KMeans(n_clusters=4) and fit it. Plot the data with colors based on cluster labels and optionally mark the centroids in the scatter plot.\n",
        "\n",
        "Ques 30: Load the Iris dataset and use Agglomerative Clustering to group the data into 3 clusters. Display the first 10 predicted labels\n",
        "\n",
        "Ans\n",
        "Load and standardize the Iris dataset. Apply AgglomerativeClustering(n_clusters=3) and fit it to the data. Print the first 10 values of model.labels_ to show the cluster assignments for the initial records.\n",
        "\n",
        "Ques 31: Generate synthetic data using make_moons and apply DBSCAN. Highlight outliers in the plot\n",
        "\n",
        "Ans\n",
        "Generate moon-shaped data using make_moons(noise=0.1). Fit DBSCAN(eps=0.2, min_samples=5) and identify outliers as points labeled -1. Plot the data using a different color or marker for noise points to highlight them.\n",
        "\n",
        "Ques 32: Load the Wine dataset and apply K-Means clustering after standardizing the features. Print the size of each cluster\n",
        "\n",
        "Ans\n",
        "Load and standardize the Wine dataset. Apply KMeans(n_clusters=3). Count the number of samples in each cluster using np.bincount(kmeans.labels_) to display the size of each group.\n",
        "\n",
        "Ques 33: Use make_circles to generate synthetic data and cluster it using DBSCAN. Plot the result\n",
        "\n",
        "Ans\n",
        "Create circular data using make_circles(noise=0.05). Apply DBSCAN(eps=0.2, min_samples=5) and plot the clusters using different colors. DBSCAN should effectively separate the two circular regions.\n",
        "\n",
        "Ques 34: Load the Breast Cancer dataset, apply MinMaxScaler, and use K-Means with 2 clusters. Output the cluster centroids\n",
        "\n",
        "Ans\n",
        "Load the Breast Cancer dataset and scale it using MinMaxScaler. Apply KMeans(n_clusters=2) and fit the data. Print kmeans.cluster_centers_ to get the centroids of the two clusters.\n",
        "\n",
        "Ques 35: Generate synthetic data using make_blobs with varying cluster standard deviations and cluster with DBSCAN\n",
        "\n",
        "Ans\n",
        "Use make_blobs with the cluster_std parameter set to different values for each cluster. Fit DBSCAN with a suitable eps and min_samples. Plot the clusters to see how DBSCAN handles the variation in density.\n",
        "\n",
        "Ques 36: Load the Digits dataset, reduce it to 2D using PCA, and visualize clusters from K-Means\n",
        "\n",
        "Ans\n",
        "Load and scale the Digits dataset. Apply PCA(n_components=2) to reduce the dimensions. Use KMeans(n_clusters=10) and plot the results in 2D using cluster labels to color the points.\n",
        "\n",
        "Ques 37: Create synthetic data using make_blobs and evaluate silhouette scores for k = 2 to 5. Display as a bar chart\n",
        "\n",
        "Ans\n",
        "Generate data using make_blobs. For k in range 2 to 5, fit KMeans and compute the silhouette score. Store the scores and plot them as a bar chart to find the optimal number of clusters based on maximum silhouette score.\n",
        "\n",
        "Ques 38: Load the Iris dataset and use hierarchical clustering to group data. Plot a dendrogram with average linkage\n",
        "\n",
        "Ans\n",
        "Load and scale the Iris dataset. Use linkage(method='average') from scipy to compute the linkage matrix. Then use dendrogram() to plot the hierarchical structure, showing how clusters are merged at each step.\n",
        "\n",
        "Ques 39: Generate synthetic data with overlapping clusters using make_blobs, then apply K-Means and visualize with decision boundaries\n",
        "\n",
        "Ans\n",
        "Create overlapping clusters with make_blobs(cluster_std=2.0). Fit KMeans(n_clusters=3). Use meshgrid and decision boundary plotting techniques to visualize how KMeans separates the overlapping areas.\n",
        "\n",
        "Ques 40: Load the Digits dataset and apply DBSCAN after reducing dimensions with t-SNE. Visualize the results\n",
        "\n",
        "Ans\n",
        "Load and standardize the Digits dataset. Apply TSNE(n_components=2) and fit DBSCAN to the reduced data. Plot the clusters using different colors and highlight noise points separately.\n",
        "\n",
        "Ques 41: Generate synthetic data using make_blobs and apply Agglomerative Clustering with complete linkage. Plot the result\n",
        "\n",
        "Ans\n",
        "Use make_blobs to generate data and apply AgglomerativeClustering(linkage='complete', n_clusters=3). Plot the results using a scatter plot to visualize how complete linkage forms the clusters based on the farthest point distances.\n",
        "\n",
        "Ques 42: Load the Breast Cancer dataset and compare inertia values for K = 2 to 6 using K-Means. Show results in a line plot\n",
        "\n",
        "Ans\n",
        "Load and scale the dataset. For k in range 2 to 6, fit KMeans(n_clusters=k) and store the inertia_ values. Plot k vs inertia to create an elbow plot to identify the optimal number of clusters.\n",
        "\n",
        "Ques 43: Generate synthetic concentric circles using make_circles and cluster using Agglomerative Clustering with single linkage\n",
        "\n",
        "Ans\n",
        "Create circular data using make_circles(noise=0.05). Apply AgglomerativeClustering(n_clusters=2, linkage='single') and plot the results. Single linkage may not perform well with such shapes, causing irregular clusters.\n",
        "\n",
        "Ques 44: Use the Wine dataset, apply DBSCAN after scaling the data, and count the number of clusters (excluding noise)\n",
        "\n",
        "Ans\n",
        "Load and scale the Wine dataset. Apply DBSCAN and extract labels_. Count the unique labels excluding -1 to determine the number of clusters found by DBSCAN.\n",
        "\n",
        "Ques 45: Generate synthetic data with make_blobs and apply KMeans. Then plot the cluster centers on top of the data points\n",
        "\n",
        "Ans\n",
        "Create data with make_blobs(n_samples=300, centers=4). Apply KMeans(n_clusters=4) and plot the data using a scatter plot. Overlay the cluster centers using a larger marker or different color.\n",
        "\n",
        "Ques 46: Load the Iris dataset, cluster with DBSCAN, and print how many samples were identified as noise\n",
        "\n",
        "Ans\n",
        "Load and standardize the Iris dataset. Apply DBSCAN and count the number of samples where label is -1 to get the count of noise points.\n",
        "\n",
        "Ques 47: Generate synthetic non-linearly separable data using make_moons, apply K-Means, and visualize the clustering result\n",
        "\n",
        "Ans\n",
        "Use make_moons(noise=0.1) to generate the data. Apply KMeans(n_clusters=2) and plot the clustering result. KMeans usually fails on such non-linear data as it assumes spherical clusters.\n",
        "\n",
        "Ques 48: Load the Digits dataset, apply PCA to reduce to 3 components, then use KMeans and visualize with a 3D scatter plot\n",
        "\n",
        "Ans\n",
        "Load and scale the Digits dataset. Apply PCA(n_components=3) and fit KMeans(n_clusters=10). Use a 3D scatter plot to visualize the clusters using the three principal components with different colors for each cluster."
      ],
      "metadata": {
        "id": "IKOrhgwJKNB6"
      }
    }
  ]
}