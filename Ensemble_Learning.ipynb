{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNIb5mbIvtpfdw3s1bL/7LC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kanika0216/python-Basics/blob/main/Ensemble_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Theoretical**"
      ],
      "metadata": {
        "id": "_9nnJMbvoAfp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bilkul! Neeche 20 questions ke answers diye gaye hain — har ek ka concise 2–3 line ka jawab diya gaya hai, question ke sath:\n",
        "\n",
        "---\n",
        "\n",
        "**1. Can we use Bagging for regression problems?**  \n",
        "Yes, bagging can be used for regression. In this case, the final output is the average of predictions from all base estimators.\n",
        "\n",
        "**2. What is the difference between multiple model training and single model training?**  \n",
        "Single model training uses one algorithm to learn from data, while multiple model training combines several models to improve performance and reduce variance or bias.\n",
        "\n",
        "**3. Explain the concept of feature randomness in Random Forest.**  \n",
        "In Random Forest, feature randomness means that each tree selects a random subset of features for splitting, which increases diversity among trees and improves generalization.\n",
        "\n",
        "**4. What is OOB (Out-of-Bag) Score?**  \n",
        "OOB score is the validation score calculated using data not included in the bootstrap sample for a particular tree, acting like built-in cross-validation for Bagging models.\n",
        "\n",
        "**5. How can you measure the importance of features in a Random Forest model?**  \n",
        "Feature importance is measured by how much each feature decreases the impurity across all trees, or by observing the effect on model accuracy when a feature's values are shuffled.\n",
        "\n",
        "**6. Explain the working principle of a Bagging Classifier.**  \n",
        "A Bagging Classifier builds multiple models on different bootstrap samples and aggregates their predictions (majority vote for classification) to form the final prediction.\n",
        "\n",
        "**7. How do you evaluate a Bagging Classifier’s performance?**  \n",
        "You can use standard metrics like accuracy, precision, recall, or ROC-AUC on a test set, or rely on the OOB score for performance estimation.\n",
        "\n",
        "**8. How does a Bagging Regressor work?**  \n",
        "A Bagging Regressor trains multiple regressors on different subsets of data and averages their predictions to produce the final output, reducing variance.\n",
        "\n",
        "**9. What is the main advantage of ensemble techniques?**  \n",
        "Ensemble techniques improve prediction accuracy and robustness by combining multiple models to offset individual model weaknesses.\n",
        "\n",
        "**10. What is the main challenge of ensemble methods?**  \n",
        "They are often computationally expensive and harder to interpret compared to single models, and may risk overfitting if not used properly.\n",
        "\n",
        "**11. Explain the key idea behind ensemble techniques.**  \n",
        "The key idea is to combine predictions from multiple models to produce a stronger, more accurate model than any individual one.\n",
        "\n",
        "**12. What is a Random Forest Classifier?**  \n",
        "It's an ensemble method that uses many decision trees built on random subsets of data and features, and outputs the majority vote as the final prediction.\n",
        "\n",
        "**13. What are the main types of ensemble techniques?**  \n",
        "The main types are Bagging, Boosting, and Stacking — each with different strategies for combining multiple models.\n",
        "\n",
        "**14. What is ensemble learning in machine learning?**  \n",
        "Ensemble learning is a technique where multiple models are combined to solve a problem and improve performance over individual models.\n",
        "\n",
        "**15. When should we avoid using ensemble methods?**  \n",
        "Avoid them when model interpretability is critical, or when the dataset is small, as they can become unnecessarily complex or overfit.\n",
        "\n",
        "**16. How does Bagging help in reducing overfitting?**  \n",
        "Bagging reduces overfitting by averaging multiple models trained on varied data, which decreases model variance and prevents fitting to noise.\n",
        "\n",
        "**17. Why is Random Forest better than a single Decision Tree?**  \n",
        "Random Forest reduces overfitting and improves accuracy by combining multiple decision trees with randomness in data and features.\n",
        "\n",
        "**18. What is the role of bootstrap sampling in Bagging?**  \n",
        "Bootstrap sampling creates different training sets by sampling with replacement, promoting diversity among models in the ensemble.\n",
        "\n",
        "**19. What are some real-world applications of ensemble techniques?**  \n",
        "Used in fraud detection, spam filtering, recommendation systems, medical diagnosis, and financial forecasting for their high accuracy.\n",
        "\n",
        "**20. What is the difference between Bagging and Boosting?**  \n",
        "Bagging builds models independently in parallel and focuses on variance reduction, while Boosting builds models sequentially, each focusing on errors of the previous, aiming to reduce bias.\n",
        "\n"
      ],
      "metadata": {
        "id": "a-U7jsV1oD71"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Practical**"
      ],
      "metadata": {
        "id": "LqyVCzSkoHO4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "21. Train a Bagging Classifier using Decision Trees on a sample dataset and print model accuracy\n"
      ],
      "metadata": {
        "id": "A4v1JHqUoKjQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Use BaggingClassifier with DecisionTreeClassifier on datasets like Iris. Accuracy can be printed using accuracy_score() on test data."
      ],
      "metadata": {
        "id": "YFsr4B-aoGa_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "22. Train a Bagging Regressor using Decision Trees and evaluate using Mean Squared Error (MSE)\n"
      ],
      "metadata": {
        "id": "o_-Zvr-zob9E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Use BaggingRegressor with DecisionTreeRegressor. Evaluate on test data using mean_squared_error() from sklearn.metrics."
      ],
      "metadata": {
        "id": "W894wr2NodIh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "23. Train a Random Forest Classifier on the Breast Cancer dataset and print feature importance scores\n"
      ],
      "metadata": {
        "id": "BLS3VWZ0odW2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Load the dataset via load_breast_cancer(), train RandomForestClassifier, and use .feature_importances_ to print the scores."
      ],
      "metadata": {
        "id": "_ATtED2Wodc2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "24. Train a Random Forest Regressor and compare its performance with a single Decision Tree\n"
      ],
      "metadata": {
        "id": "K4Gxeh5OodiZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Train both RandomForestRegressor and DecisionTreeRegressor on the same dataset, and compare using metrics like MSE or R²."
      ],
      "metadata": {
        "id": "9SAxfeY-odn0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "25. Compute the Out-of-Bag (OOB) Score for a Random Forest Classifier"
      ],
      "metadata": {
        "id": "FUZMOksaodtH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Set oob_score=True in RandomForestClassifier while training. Access the score via .oob_score_ to evaluate model performance."
      ],
      "metadata": {
        "id": "ij3tXztDodyO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "26. Train a Bagging Classifier using SVM as a base estimator and print accuracy"
      ],
      "metadata": {
        "id": "bg8FrNOfod3g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Use BaggingClassifier(base_estimator=SVC()). Train it on a dataset like digits or Iris, and evaluate with accuracy_score()."
      ],
      "metadata": {
        "id": "Nyx1-OHPod8Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "27. Train a Random Forest Classifier with different numbers of trees and compare accuracy\n"
      ],
      "metadata": {
        "id": "taJfWTYSoeAs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Train RandomForestClassifier with n_estimators=[10, 50, 100, 200]. Record accuracy for each and plot or compare manually."
      ],
      "metadata": {
        "id": "gAHS9NqkoeFN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "28. Train a Bagging Classifier using Logistic Regression as a base estimator and print AUC score\n"
      ],
      "metadata": {
        "id": "zMhOLPmtoeJf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Use BaggingClassifier(base_estimator=LogisticRegression()). Evaluate using roc_auc_score() from sklearn.metrics."
      ],
      "metadata": {
        "id": "yevilb4goeN1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "29. Train a Random Forest Regressor and analyze feature importance scores"
      ],
      "metadata": {
        "id": "6YgQHC-koeS1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "After training the model, use .feature_importances_ to see which features contribute most to prediction."
      ],
      "metadata": {
        "id": "9PBB1PGFoeXO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "30. Train an ensemble model using both Bagging and Random Forest and compare accuracy\n"
      ],
      "metadata": {
        "id": "GzW90hg8oeb6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Train both models (BaggingClassifier and RandomForestClassifier) on same data. Use accuracy_score() on test set to compare."
      ],
      "metadata": {
        "id": "6ngmGGnmoeg7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "31. Train a Random Forest Classifier and tune hyperparameters using GridSearchCV\n"
      ],
      "metadata": {
        "id": "icKaXIxnoelf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Use GridSearchCV with RandomForestClassifier to optimize parameters like n_estimators, max_depth, and min_samples_split."
      ],
      "metadata": {
        "id": "9stJVAR6oeqB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "32. Train a Bagging Regressor with different numbers of base estimators and compare performance"
      ],
      "metadata": {
        "id": "03f7Nl1moeua"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Vary n_estimators in BaggingRegressor, train on same dataset, and compare performance using metrics like MSE or R²."
      ],
      "metadata": {
        "id": "IkHmaefDoezm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "33. Train a Random Forest Classifier and analyze misclassified samples"
      ],
      "metadata": {
        "id": "csDMWvsCoe5C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "After prediction, compare y_test with y_pred, identify mismatches, and analyze features that caused the misclassification."
      ],
      "metadata": {
        "id": "5--VIIIUofDJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "34. Train a Bagging Classifier and compare its performance with a single Decision Tree Classifier"
      ],
      "metadata": {
        "id": "DGvhi8nspQOM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Train both classifiers on same dataset, compare accuracy_score, and observe how Bagging reduces variance and overfitting."
      ],
      "metadata": {
        "id": "a0F5XHIspQea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "35. Train a Random Forest Classifier and visualize the confusion matrix"
      ],
      "metadata": {
        "id": "UpaEjAMvpQj9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Use confusion_matrix() from sklearn.metrics, then visualize using ConfusionMatrixDisplay or seaborn.heatmap."
      ],
      "metadata": {
        "id": "atQH7wdKpQqr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "36. Train a Stacking Classifier using Decision Trees, SVM, and Logistic Regression, and compare accuracy"
      ],
      "metadata": {
        "id": "PF6D4Y-UpQwL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Use StackingClassifier with these base learners and compare its accuracy against individual models on the same test set."
      ],
      "metadata": {
        "id": "hYJNqEtEpQ2F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "37. Train a Random Forest Classifier and print the top 5 most important features"
      ],
      "metadata": {
        "id": "RRSkdEP5pQ7f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "After training, sort feature_importances_ in descending order and print the top 5 contributing features."
      ],
      "metadata": {
        "id": "vTUT3PHWpRA4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "38. Train a Bagging Classifier and evaluate performance using Precision, Recall, and F1-score"
      ],
      "metadata": {
        "id": "JQS2spzwpRGe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Use precision_score, recall_score, and f1_score from sklearn.metrics to evaluate your Bagging Classifier."
      ],
      "metadata": {
        "id": "zrZREzjfpRMI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "39. Train a Random Forest Classifier and analyze the effect of max_depth on accuracy"
      ],
      "metadata": {
        "id": "zjw_AbntpRR3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Train models with varying max_depth values, record accuracies, and plot to observe the impact on model performance."
      ],
      "metadata": {
        "id": "8luxYL-upRYh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "40. Train a Bagging Regressor using different base estimators (DecisionTree and KNeighbors) and compare performance"
      ],
      "metadata": {
        "id": "k9tt5tDOpRtQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Use BaggingRegressor with DecisionTreeRegressor and KNeighborsRegressor, then compare results using MSE or R²."
      ],
      "metadata": {
        "id": "sD4dA_38pR1O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "41. Train a Random Forest Classifier and evaluate its performance using ROC-AUC Score"
      ],
      "metadata": {
        "id": "5aotW_nnpR8B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Use roc_auc_score() on predicted probabilities (predict_proba()) to evaluate the classifier's AUC."
      ],
      "metadata": {
        "id": "swafU649pSC2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "42. Train a Bagging Classifier and evaluate its performance using cross-validation"
      ],
      "metadata": {
        "id": "tTWJcnFOpSI4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Use cross_val_score() with BaggingClassifier to evaluate average model performance across folds."
      ],
      "metadata": {
        "id": "Tmex-Yd_pSOS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "43. Train a Random Forest Classifier and plot the Precision-Recall curve"
      ],
      "metadata": {
        "id": "2lv95P-5ps10"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Use precision_recall_curve() and matplotlib or seaborn to visualize how precision and recall vary with thresholds."
      ],
      "metadata": {
        "id": "EiKT_2GVptjQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "44. Train a Stacking Classifier with Random Forest and Logistic Regression and compare accuracy"
      ],
      "metadata": {
        "id": "yFB3HzBDptqR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Use StackingClassifier with these models, evaluate on test set, and compare accuracy with standalone versions."
      ],
      "metadata": {
        "id": "vyjkP3CaptwB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "45. Train a Bagging Regressor with different levels of bootstrap samples and compare performance\n"
      ],
      "metadata": {
        "id": "BT9m-vb8pt07"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Adjust max_samples in BaggingRegressor to control bootstrap sample size, and compare MSE or R² across versions."
      ],
      "metadata": {
        "id": "r5K96S1opt58"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}