{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN5MjaH7RGMoi8KGdBWojsk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kanika0216/python-Basics/blob/main/Decision_Tree.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "                                            **Theoretical**"
      ],
      "metadata": {
        "id": "ss0ynsX9Aor_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is a Decision Tree, and how does it work?\n",
        "\n",
        "A Decision Tree is a flowchart-like structure used for classification or regression, where data is split based on feature values to reach a decision.\n",
        "\n",
        "2. What are impurity measures in Decision Trees?\n",
        "\n",
        "Impurity measures quantify how mixed the classes are at a node, guiding the best feature splits.\n",
        "\n",
        "3. What is the mathematical formula for Gini Impurity?\n",
        "\n",
        "Gini = 1 − ∑(pᵢ²), where pᵢ is the probability of class i at a node.\n",
        "\n",
        "4. What is the mathematical formula for Entropy?\n",
        "\n",
        "Entropy = −∑(pᵢ * log₂(pᵢ)), where pᵢ is the probability of class i.\n",
        "\n",
        "5. What is Information Gain, and how is it used in Decision Trees?\n",
        "\n",
        "Information Gain is the reduction in impurity after a split; it's used to choose the best feature at each node.\n",
        "\n",
        "6. What is the difference between Gini Impurity and Entropy?\n",
        "\n",
        "Both measure impurity, but Gini is faster to compute; Entropy is based on information theory and can be more precise.\n",
        "\n",
        "7. What is the mathematical explanation behind Decision Trees?\n",
        "\n",
        "They use recursive binary splitting by minimizing impurity (Gini/Entropy) and selecting optimal features.\n",
        "\n",
        "8. What is Pre-Pruning in Decision Trees?\n",
        "\n",
        "Pre-pruning stops tree growth early based on conditions like max depth or min samples per node.\n",
        "\n",
        "9. What is Post-Pruning in Decision Trees?\n",
        "\n",
        "Post-pruning grows the full tree first, then trims nodes that don't improve accuracy on validation data.\n",
        "\n",
        "10. What is the difference between Pre-Pruning and Post-Pruning?\n",
        "\n",
        "Pre-pruning prevents overfitting during training; post-pruning removes overfitting after full growth.\n",
        "\n",
        "11. What is a Decision Tree Regressor?\n",
        "\n",
        "It’s a Decision Tree used for predicting continuous values by splitting data to minimize variance.\n",
        "\n",
        "12. What are the advantages and disadvantages of Decision Trees?\n",
        "\n",
        "They're easy to interpret and handle all data types, but can overfit and be sensitive to small data changes.\n",
        "\n",
        "13. How does a Decision Tree handle missing values?\n",
        "\n",
        "It can use surrogate splits or ignore missing values during training depending on the implementation.\n",
        "\n",
        "14. How does a Decision Tree handle categorical features?\n",
        "\n",
        "It splits based on different category combinations or uses one-vs-rest strategies.\n",
        "\n",
        "15. What are some real-world applications of Decision Trees?\n",
        "\n",
        "Used in medical diagnosis, credit scoring, fraud detection, customer segmentation, and more.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "46AYWQHjA4sk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "                                          **Practical**"
      ],
      "metadata": {
        "id": "zgQPmA6vCAs3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. Write a Python program to train a Decision Tree Classifier on the Iris dataset and print the model accuracy."
      ],
      "metadata": {
        "id": "BPWzJL6lCtHN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X, y = load_iris(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
        "\n",
        "clf = DecisionTreeClassifier()\n",
        "clf.fit(X_train, y_train)\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tCtn5upPCwu0",
        "outputId": "c1e59818-84d6-48ee-f8fe-651c33267bb7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9736842105263158\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. Write a Python program to train a Decision Tree Classifier using Gini Impurity as the criterion and print the\n",
        "feature importances"
      ],
      "metadata": {
        "id": "s-9ySKkfC03h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clf = DecisionTreeClassifier(criterion='gini')\n",
        "clf.fit(X_train, y_train)\n",
        "print(\"Feature importances:\", clf.feature_importances_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AkDPMRFxC5za",
        "outputId": "6ed27a5f-acb2-4963-85c1-b1e2fdbb128c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature importances: [0.         0.04029743 0.90597266 0.05372991]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. Write a Python program to train a Decision Tree Classifier using Entropy as the splitting criterion and print the\n",
        "model accuracy."
      ],
      "metadata": {
        "id": "gdwk9esBDC8c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clf = DecisionTreeClassifier(criterion='entropy')\n",
        "clf.fit(X_train, y_train)\n",
        "y_pred = clf.predict(X_test)\n",
        "print(\"Accuracy (Entropy):\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "qtHOg85TDKMr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. Write a Python program to train a Decision Tree Regressor on a housing dataset and evaluate using Mean\n",
        "Squared Error (MSE)."
      ],
      "metadata": {
        "id": "P_-_yV7nDNwN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "X, y = fetch_california_housing(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
        "\n",
        "reg = DecisionTreeRegressor()\n",
        "reg.fit(X_train, y_train)\n",
        "y_pred = reg.predict(X_test)\n",
        "\n",
        "print(\"MSE:\", mean_squared_error(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "0ClxrbXMDRpW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. Write a Python program to train a Decision Tree Classifier and visualize the tree using graphviz."
      ],
      "metadata": {
        "id": "Y0Y-2hnZDUXf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import export_graphviz\n",
        "import graphviz\n",
        "\n",
        "clf = DecisionTreeClassifier()\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "dot_data = export_graphviz(clf, out_file=None, feature_names=load_iris().feature_names, class_names=load_iris().target_names, filled=True)\n",
        "graph = graphviz.Source(dot_data)\n",
        "graph.render(\"iris_tree\")  # Saves as iris_tree.pdf\n"
      ],
      "metadata": {
        "id": "XwSVaPd4DXs7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "21. Write a Python program to train a Decision Tree Classifier with a maximum depth of 3 and compare its\n",
        "accuracy with a fully grown tree."
      ],
      "metadata": {
        "id": "wFClMEkCDaLZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "shallow = DecisionTreeClassifier(max_depth=3)\n",
        "shallow.fit(X_train, y_train)\n",
        "deep = DecisionTreeClassifier()\n",
        "deep.fit(X_train, y_train)\n",
        "\n",
        "print(\"Shallow Tree Accuracy:\", accuracy_score(y_test, shallow.predict(X_test)))\n",
        "print(\"Full Tree Accuracy:\", accuracy_score(y_test, deep.predict(X_test)))\n"
      ],
      "metadata": {
        "id": "kyk-O08mDd4Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "22. Write a Python program to train a Decision Tree Classifier using min_samples_split=5 and compare its\n",
        "accuracy with a default tree."
      ],
      "metadata": {
        "id": "F3LZ6hcSDlMB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "custom = DecisionTreeClassifier(min_samples_split=5)\n",
        "custom.fit(X_train, y_train)\n",
        "default = DecisionTreeClassifier()\n",
        "default.fit(X_train, y_train)\n",
        "\n",
        "print(\"Custom Tree Accuracy:\", accuracy_score(y_test, custom.predict(X_test)))\n",
        "print(\"Default Tree Accuracy:\", accuracy_score(y_test, default.predict(X_test)))\n"
      ],
      "metadata": {
        "id": "8g3jlbVBDoer"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "23. Write a Python program to apply feature scaling before training a Decision Tree Classifier and compare its\n",
        "accuracy with unscaled data."
      ],
      "metadata": {
        "id": "ThsXn2sKDq0O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, random_state=0)\n",
        "scaled_clf = DecisionTreeClassifier()\n",
        "scaled_clf.fit(X_train, y_train)\n",
        "\n",
        "unscaled_clf = DecisionTreeClassifier()\n",
        "unscaled_clf.fit(*train_test_split(X, y, random_state=0)[:2])\n",
        "\n",
        "print(\"Scaled Accuracy:\", accuracy_score(y_test, scaled_clf.predict(X_test)))\n",
        "print(\"Unscaled Accuracy:\", accuracy_score(y_test, unscaled_clf.predict(X_test)))\n"
      ],
      "metadata": {
        "id": "uIXhCqdvDvTL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "24. Write a Python program to train a Decision Tree Classifier using One-vs-Rest (OvR) strategy for multiclass\n",
        "classification"
      ],
      "metadata": {
        "id": "2R-AixJxDx1h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "\n",
        "ovr_clf = OneVsRestClassifier(DecisionTreeClassifier())\n",
        "ovr_clf.fit(X_train, y_train)\n",
        "print(\"OvR Accuracy:\", accuracy_score(y_test, ovr_clf.predict(X_test)))\n"
      ],
      "metadata": {
        "id": "-P0ECffhD2Ez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "25. Write a Python program to train a Decision Tree Classifier and display the feature importance scores."
      ],
      "metadata": {
        "id": "A4B56ZPlD5Ls"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clf = DecisionTreeClassifier()\n",
        "clf.fit(X_train, y_train)\n",
        "print(\"Feature Importances:\", clf.feature_importances_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2tRr4KXXD9tW",
        "outputId": "6f9c3f83-6dda-4c9c-c9c2-f242ff7d54ff"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Importances: [0.         0.02014872 0.89994526 0.07990602]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "26. Write a Python program to train a Decision Tree Regressor with max_depth=5 and compare its performance\n",
        "with an unrestricted tree."
      ],
      "metadata": {
        "id": "EI7CjAYWEBzh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reg_small = DecisionTreeRegressor(max_depth=5)\n",
        "reg_small.fit(X_train, y_train)\n",
        "reg_full = DecisionTreeRegressor()\n",
        "reg_full.fit(X_train, y_train)\n",
        "\n",
        "print(\"MSE (Depth 5):\", mean_squared_error(y_test, reg_small.predict(X_test)))\n",
        "print(\"MSE (Full):\", mean_squared_error(y_test, reg_full.predict(X_test)))\n"
      ],
      "metadata": {
        "id": "7EsUWgYrEGn6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "27. Write a Python program to train a Decision Tree Classifier, apply Cost Complexity Pruning (CCP), and\n",
        "visualize its effect on accuracy."
      ],
      "metadata": {
        "id": "dcfPit97EOQt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = clf.cost_complexity_pruning_path(X_train, y_train)\n",
        "ccp_alphas = path.ccp_alphas\n",
        "\n",
        "for ccp_alpha in ccp_alphas:\n",
        "    pruned_tree = DecisionTreeClassifier(ccp_alpha=ccp_alpha)\n",
        "    pruned_tree.fit(X_train, y_train)\n",
        "    acc = accuracy_score(y_test, pruned_tree.predict(X_test))\n",
        "    print(f\"Alpha: {ccp_alpha:.4f}, Accuracy: {acc:.4f}\")\n"
      ],
      "metadata": {
        "id": "KCHyyghvEzAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "28. Write a Python program to train a Decision Tree Classifier and evaluate its performance using Precision,\n",
        "Recall, and F1-Score"
      ],
      "metadata": {
        "id": "JMToFBJFET2V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "y_pred = clf.predict(X_test)\n",
        "print(\"Precision:\", precision_score(y_test, y_pred, average='macro'))\n",
        "print(\"Recall:\", recall_score(y_test, y_pred, average='macro'))\n",
        "print(\"F1 Score:\", f1_score(y_test, y_pred, average='macro'))\n"
      ],
      "metadata": {
        "id": "UhnCmDYkEySM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "29. Write a Python program to train a Decision Tree Classifier and visualize the confusion matrix using seaborn."
      ],
      "metadata": {
        "id": "9mR7C6ODE5aB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "sns.heatmap(cm, annot=True, cmap=\"Blues\", xticklabels=load_iris().target_names, yticklabels=load_iris().target_names)\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "vi1JzRJcE9Jc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "30. Write a Python program to train a Decision Tree Classifier and use GridSearchCV to find the optimal values\n",
        "for max_depth and min_samples_split."
      ],
      "metadata": {
        "id": "zgQg2J5xE_g7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = {\n",
        "    'max_depth': [2, 3, 4, 5, None],\n",
        "    'min_samples_split': [2, 5, 10]\n",
        "}\n",
        "grid = GridSearchCV(DecisionTreeClassifier(), param_grid, cv=5)\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best parameters:\", grid.best_params_)\n",
        "print(\"Best accuracy:\", grid.best_score_)\n"
      ],
      "metadata": {
        "id": "961Lf5dMFEqz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}